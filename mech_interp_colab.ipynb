{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MECH_INTERP_PHYSICS_REASONING - Google Colab Evaluation\n",
    "\n",
    "This notebook allows you to run the PaliGemma evaluation on Google Colab with GPU support.\n",
    "\n",
    "## ⚠️ IMPORTANT: Setup GPU Runtime First!\n",
    "\n",
    "Before running this notebook:\n",
    "1. Go to `Runtime` → `Change runtime type`\n",
    "2. Select `GPU` as Hardware accelerator (T4 is fine)\n",
    "3. Click `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected! Please enable GPU in Runtime settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to save results or access data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Repository\n",
    "\n",
    "Choose one of the following methods to get your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Upload files directly\n",
    "# You can drag and drop your project folder into the Colab file browser\n",
    "\n",
    "# Method 2: Clone from GitHub (uncomment and modify)\n",
    "# !git clone https://github.com/YOUR_USERNAME/mech_interp_physics_reasoning.git\n",
    "# %cd mech_interp_physics_reasoning\n",
    "\n",
    "# Method 3: Copy from Google Drive (uncomment and modify)\n",
    "# !cp -r /content/drive/MyDrive/mech_interp_physics_reasoning .\n",
    "# %cd mech_interp_physics_reasoning\n",
    "\n",
    "# Method 4: Upload a zip file\n",
    "from google.colab import files\n",
    "print(\"Upload your project as a zip file:\")\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    !unzip -q *.zip\n",
    "    # Change to the project directory\n",
    "    import os\n",
    "    dirs = [d for d in os.listdir('.') if os.path.isdir(d) and 'mech_interp' in d]\n",
    "    if dirs:\n",
    "        %cd {dirs[0]}\n",
    "    !ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision transformers>=4.36.0\n",
    "!pip install -q peft accelerate bitsandbytes\n",
    "!pip install -q Pillow numpy pyyaml\n",
    "!pip install -q wandb pytz\n",
    "\n",
    "# Verify installations\n",
    "import transformers\n",
    "import peft\n",
    "print(f\"✓ Transformers version: {transformers.__version__}\")\n",
    "print(f\"✓ PEFT version: {peft.__version__}\")\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check and Fix Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current directory structure\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the test_size issue in config\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "config_path = \"base_eval_config.yaml\"\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    # Read current config\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    print(f\"Current test_size: {config['data_config']['test_size']}\")\n",
    "\n",
    "    # Update test_size if needed\n",
    "    if config['data_config']['test_size'] < 10:\n",
    "        config['data_config']['test_size'] = 100  # or 0.2 for 20%\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"✓ Updated test_size to: {config['data_config']['test_size']}\")\n",
    "    else:\n",
    "        print(f\"✓ test_size is already set to: {config['data_config']['test_size']}\")\n",
    "else:\n",
    "    print(f\"❌ Config file not found at {config_path}\")\n",
    "    print(\"Available files:\")\n",
    "    !ls -la *.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Colab-Optimized Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/eval_colab.py\n",
    "import os\n",
    "# Use GPU 0 in Colab\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import TrainingArguments\n",
    "from peft import PeftModel\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_device_map():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate a PaliGemma model on CLEVRER test set.\")\n",
    "    parser.add_argument(\"checkpoint_dir\", type=str, nargs=\"?\", help=\"Relative path to checkpoint folder\")\n",
    "    parser.add_argument(\"--base\", action=\"store_true\", help=\"Evaluate base model without LoRA\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    home_dir = os.environ.get(\"HOME_DIR\", os.path.abspath(os.path.join(script_dir, \"..\")))\n",
    "    HOME_DIR = home_dir\n",
    "    sys.path.insert(0, HOME_DIR)\n",
    "\n",
    "    if args.base:\n",
    "        config_path = os.path.join(home_dir, \"base_eval_config.yaml\")\n",
    "    else:\n",
    "        if not args.checkpoint_dir:\n",
    "            raise ValueError(\"checkpoint_dir must be provided unless --base is used\")\n",
    "        checkpoint_dir = os.path.join(home_dir, args.checkpoint_dir)\n",
    "        config_path = os.path.join(checkpoint_dir, \"config.yaml\")\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    from src.processing_paligemma import PaliGemmaProcessor\n",
    "    from src.modeling_paligemma import PaliGemmaForConditionalGeneration\n",
    "    from src.utils import make_clevrer_collate_fn, compute_accuracy, CLEVRERTrainer\n",
    "    from src.data import ClevrerDataset\n",
    "\n",
    "    model_config = config[\"model_train\"]\n",
    "    data_config = config[\"data_config\"]\n",
    "    model_id = model_config[\"model\"]\n",
    "    question_type = data_config.get(\"question_type\", \"all\")\n",
    "\n",
    "    test_frames_dir = os.path.join(HOME_DIR, data_config.get(\"data_path\", \"test_frames\"))\n",
    "    annotations_path = os.path.join(HOME_DIR, data_config.get(\"json_path\", \"miscellaneous/validation.json\"))\n",
    "\n",
    "    print(f\"Loading dataset from: {test_frames_dir}\")\n",
    "    print(f\"Annotations from: {annotations_path}\")\n",
    "\n",
    "    dataset = ClevrerDataset(\n",
    "        frames_root=test_frames_dir,\n",
    "        json_path=annotations_path,\n",
    "        question_type=question_type,\n",
    "        transform=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if args.base:\n",
    "        results_root = os.path.join(HOME_DIR, \"artifacts\", \"BASE\", f\"eval_{question_type}_{timestamp}\")\n",
    "    else:\n",
    "        results_root = os.path.join(checkpoint_dir, f\"eval_{question_type}_{timestamp}\")\n",
    "\n",
    "    os.makedirs(results_root, exist_ok=True)\n",
    "    split_cache_path = os.path.join(results_root, \"split_indices.json\")\n",
    "\n",
    "    train_ds, test_ds = dataset.train_test_split(\n",
    "        test_size=data_config['test_size'],\n",
    "        cache_path=split_cache_path\n",
    "    )\n",
    "\n",
    "    print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "    device = get_device_map()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    if args.base:\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "            attn_implementation=\"eager\",\n",
    "            token_compression=model_config.get('token_compression')\n",
    "        )\n",
    "    else:\n",
    "        base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "            attn_implementation=\"eager\",\n",
    "            token_compression=model_config.get(\"token_compression\"),\n",
    "            target_length=model_config.get(\"target_length\")\n",
    "        )\n",
    "        last_checkpoint = glob.glob(os.path.join(checkpoint_dir, \"checkpoint-*\"))[-1]\n",
    "        model = PeftModel.from_pretrained(base_model, last_checkpoint)\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "    collate_fn = make_clevrer_collate_fn(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        model_config=model_config,\n",
    "        data_config=data_config,\n",
    "        dtype=model.dtype\n",
    "    )\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"/tmp/eval\",\n",
    "        per_device_eval_batch_size=model_config.get(\"eval_batch_size\", 4),\n",
    "        eval_accumulation_steps=1,\n",
    "        dataloader_pin_memory=False,\n",
    "        bf16=True if device == \"cuda\" else False,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "        save_strategy=\"no\",\n",
    "        logging_strategy=\"no\"\n",
    "    )\n",
    "\n",
    "    trainer = CLEVRERTrainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_ds,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_accuracy,\n",
    "        processing_class=processor\n",
    "    )\n",
    "\n",
    "    print(\"Starting evaluation...\")\n",
    "    pred_output = trainer.predict(test_ds)\n",
    "    \n",
    "    # Process results\n",
    "    preds = pred_output.predictions.tolist()\n",
    "    labels = pred_output.label_ids.tolist()\n",
    "\n",
    "    special_tokens = {0, 1, 2, 3, -100, 257152}\n",
    "    correct_flags = []\n",
    "    per_sample_results = []\n",
    "    type_correct = defaultdict(int)\n",
    "    type_total = defaultdict(int)\n",
    "\n",
    "    for i, (pred_row, label_row) in enumerate(zip(preds, labels)):\n",
    "        item = test_ds[i]\n",
    "        qtype = item[\"question_type\"]\n",
    "\n",
    "        filtered_pred = [x for x in pred_row if x not in special_tokens]\n",
    "        filtered_label = [x for x in label_row if x not in special_tokens]\n",
    "\n",
    "        correct = sorted(filtered_pred) == sorted(filtered_label)\n",
    "        correct_flags.append(correct)\n",
    "\n",
    "        type_total[qtype] += 1\n",
    "        type_correct[qtype] += int(correct)\n",
    "\n",
    "        per_sample_results.append({\n",
    "            \"question_type\": qtype,\n",
    "            \"question_id\": item[\"question_id\"],\n",
    "            \"video_filename\": item[\"video_filename\"],\n",
    "            \"predicted_token_ids\": filtered_pred,\n",
    "            \"label_token_ids\": filtered_label,\n",
    "            \"correct\": correct\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    results_file = os.path.join(results_root, \"eval_results.txt\")\n",
    "    details_path = os.path.join(results_root, \"eval_details.json\")\n",
    "    \n",
    "    with open(details_path, \"w\") as f:\n",
    "        json.dump(per_sample_results, f, indent=2)\n",
    "\n",
    "    accuracy = np.mean(correct_flags) if correct_flags else 0.0\n",
    "    with open(results_file, \"w\") as f:\n",
    "        f.write(f\"accuracy: {accuracy:.4f}\\n\\n\")\n",
    "        f.write(\"Question Type Accuracies:\\n\")\n",
    "        for qtype, total in type_total.items():\n",
    "            acc = type_correct[qtype] / total if total > 0 else 0.0\n",
    "            f.write(f\"{qtype}: {acc:.4f} ({type_correct[qtype]}/{total})\\n\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluation Complete!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nQuestion Type Accuracies:\")\n",
    "    for qtype, total in type_total.items():\n",
    "        acc = type_correct[qtype] / total if total > 0 else 0.0\n",
    "        print(f\"  {qtype}: {acc:.4f} ({type_correct[qtype]}/{total})\")\n",
    "    print(f\"\\nResults saved to: {results_root}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data directories exist\n",
    "import os\n",
    "\n",
    "print(\"Checking data directories...\")\n",
    "data_dirs = ['test_frames', 'train_frames', 'miscellaneous']\n",
    "for dir_name in data_dirs:\n",
    "    if os.path.exists(dir_name):\n",
    "        count = len(os.listdir(dir_name))\n",
    "        print(f\"✓ {dir_name}: {count} items\")\n",
    "    else:\n",
    "        print(f\"❌ {dir_name}: NOT FOUND\")\n",
    "\n",
    "# Check for annotation files\n",
    "ann_files = ['miscellaneous/validation.json', 'miscellaneous/train.json']\n",
    "for ann_file in ann_files:\n",
    "    if os.path.exists(ann_file):\n",
    "        print(f\"✓ {ann_file}: EXISTS\")\n",
    "    else:\n",
    "        print(f\"❌ {ann_file}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run base model evaluation\n",
    "!python scripts/eval_colab.py --base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a checkpoint to evaluate, run this instead:\n",
    "# !python scripts/eval_colab.py path/to/your/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find the latest results\n",
    "result_dirs = glob.glob(\"artifacts/BASE/eval_*\")\n",
    "if result_dirs:\n",
    "    latest_dir = max(result_dirs, key=os.path.getctime)\n",
    "    print(f\"Latest results directory: {latest_dir}\")\n",
    "    \n",
    "    # Display text results\n",
    "    results_file = os.path.join(latest_dir, \"eval_results.txt\")\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\"*50)\n",
    "            print(f.read())\n",
    "    \n",
    "    # Load and analyze detailed results\n",
    "    details_file = os.path.join(latest_dir, \"eval_details.json\")\n",
    "    if os.path.exists(details_file):\n",
    "        with open(details_file, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        print(f\"\\nTotal samples evaluated: {len(details)}\")\n",
    "        \n",
    "        # Show some sample predictions\n",
    "        print(\"\\nSample predictions (first 5):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, result in enumerate(details[:5]):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"  Video: {result['video_filename']}\")\n",
    "            print(f\"  Question Type: {result['question_type']}\")\n",
    "            print(f\"  Correct: {'✓' if result['correct'] else '✗'}\")\n",
    "            if not result['correct']:\n",
    "                print(f\"  Predicted tokens: {result['predicted_token_ids'][:10]}...\")\n",
    "                print(f\"  Expected tokens: {result['label_token_ids'][:10]}...\")\n",
    "else:\n",
    "    print(\"No results found yet. Run the evaluation first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Google Drive\n",
    "import shutil\n",
    "\n",
    "if 'drive' in globals() and result_dirs:\n",
    "    save_path = \"/content/drive/MyDrive/mech_interp_results\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    latest_dir = max(result_dirs, key=os.path.getctime)\n",
    "    dest_dir = os.path.join(save_path, os.path.basename(latest_dir))\n",
    "    \n",
    "    shutil.copytree(latest_dir, dest_dir, dirs_exist_ok=True)\n",
    "    print(f\"✓ Results saved to Google Drive: {dest_dir}\")\n",
    "else:\n",
    "    print(\"Google Drive not mounted or no results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM) Error\n",
    "If you encounter OOM errors, try these solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Reduce batch size\n",
    "import yaml\n",
    "\n",
    "with open('base_eval_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set smaller batch size\n",
    "config['model_train']['eval_batch_size'] = 1\n",
    "\n",
    "with open('base_eval_config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"✓ Batch size reduced to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"✓ GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single sample to debug\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.data import ClevrerDataset\n",
    "\n",
    "dataset = ClevrerDataset(\n",
    "    frames_root=\"test_frames\",\n",
    "    json_path=\"miscellaneous/validation.json\",\n",
    "    question_type=\"descriptive\"\n",
    ")\n",
    "\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset[0]\n",
    "    print(f\"Sample question: {sample['question']}\")\n",
    "    print(f\"Expected answer: {sample['answer']}\")\n",
    "    print(f\"Question type: {sample['question_type']}\")\n",
    "    print(f\"Number of frames: {len(sample['frames'])}\")\n",
    "else:\n",
    "    print(\"Dataset is empty!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
