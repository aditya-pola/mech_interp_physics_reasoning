{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MECH_INTERP_PHYSICS_REASONING - Kaggle Evaluation\n",
    "\n",
    "This notebook allows you to run the PaliGemma evaluation on Kaggle with GPU support.\n",
    "\n",
    "## ⚠️ IMPORTANT: Setup GPU Runtime First!\n",
    "\n",
    "Before running this notebook:\n",
    "1. In the Kaggle Notebook editor, go to `Settings` (right sidebar).\n",
    "2. Under `Accelerator`, select `GPU` (e.g., T4 x2, P100).\n",
    "3. Ensure your Code and Data Kaggle Datasets are added via `File` -> `Add or upload data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected or CUDA not available! Please enable GPU in Notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Project Environment\n",
    "\n",
    "Assuming your project code (including this notebook, `scripts/`, `src/`, `base_eval_config.yaml`) is in a Kaggle Dataset named `mech-interp-project-code`.\n",
    "And your data (`test_frames/`, `miscellaneous/`) is in a Kaggle Dataset named `mech-interp-clevrer-data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- IMPORTANT: Adjust these paths if your Kaggle dataset names are different ---\n",
    "KAGGLE_CODE_DATASET_PATH = '/kaggle/input/mech-interp-project-code/' # Root of your code dataset\n",
    "KAGGLE_DATA_DATASET_PATH = '/kaggle/input/mech-interp-clevrer-data/' # Root of your data dataset\n",
    "# ---\n",
    "\n",
    "# Change working directory to the root of your project code\n",
    "if os.path.exists(KAGGLE_CODE_DATASET_PATH):\n",
    "    os.chdir(KAGGLE_CODE_DATASET_PATH)\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"ERROR: Code dataset path not found: {KAGGLE_CODE_DATASET_PATH}\")\n",
    "    print(\"Please ensure you've added your code dataset and the path is correct.\")\n",
    "\n",
    "print(\"\\nCurrent directory listing:\")\n",
    "!ls -la\n",
    "\n",
    "# Define project root for scripts (should be the current KAGGLE_CODE_DATASET_PATH)\n",
    "os.environ['HOME_DIR'] = os.getcwd() \n",
    "print(f\"HOME_DIR environment variable set to: {os.environ['HOME_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision transformers>=4.36.0\n",
    "!pip install -q peft accelerate bitsandbytes\n",
    "!pip install -q Pillow numpy pyyaml\n",
    "!pip install -q wandb pytz # wandb and pytz might not be strictly needed for eval but kept from original\n",
    "\n",
    "# Verify installations\n",
    "import transformers\n",
    "import peft\n",
    "print(f\"✓ Transformers version: {transformers.__version__}\")\n",
    "print(f\"✓ PEFT version: {peft.__version__}\")\n",
    "print(\"✓ Key packages queryable after installation attempt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check and Update Configuration for Kaggle\n",
    "\n",
    "This step updates `base_eval_config.yaml` to point to data paths within your Kaggle data dataset and sets `test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import json\n",
    "\n",
    "config_filename = \"base_eval_config.yaml\"\n",
    "# Config path is now relative to KAGGLE_CODE_DATASET_PATH (current working directory)\n",
    "config_path_abs = os.path.join(os.getcwd(), config_filename)\n",
    "\n",
    "if os.path.exists(config_path_abs):\n",
    "    print(f\"Found config file: {config_path_abs}\")\n",
    "    # Read current config\n",
    "    with open(config_path_abs, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    print(f\"Original test_size: {config['data_config'].get('test_size', 'Not set')}\")\n",
    "    print(f\"Original data_path: {config['data_config'].get('data_path', 'Not set')}\")\n",
    "    print(f\"Original json_path: {config['data_config'].get('json_path', 'Not set')}\")\n",
    "\n",
    "    # Update data paths to point to the Kaggle data dataset\n",
    "    # KAGGLE_DATA_DATASET_PATH was defined in cell \"2. Setup Project Environment\"\n",
    "    config['data_config']['data_path'] = os.path.join(KAGGLE_DATA_DATASET_PATH, 'test_frames')\n",
    "    config['data_config']['json_path'] = os.path.join(KAGGLE_DATA_DATASET_PATH, 'miscellaneous/validation.json')\n",
    "\n",
    "    # Update test_size: evaluate on all samples in validation.json or a fixed number\n",
    "    # Option 1: Evaluate on all samples from validation.json\n",
    "    abs_validation_json_path = config['data_config']['json_path']\n",
    "    if os.path.exists(abs_validation_json_path):\n",
    "        with open(abs_validation_json_path, 'r') as ann_file:\n",
    "            num_samples = len(json.load(ann_file))\n",
    "        config['data_config']['test_size'] = num_samples \n",
    "        print(f\"✓ Set test_size to evaluate all {num_samples} samples from {abs_validation_json_path}.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Validation JSON not found at {abs_validation_json_path}. test_size not updated dynamically. Ensure KAGGLE_DATA_DATASET_PATH is correct.\")\n",
    "        # Fallback or keep original if file not found, e.g.:\n",
    "        # config['data_config']['test_size'] = config['data_config'].get('test_size', 100) # or a default\n",
    "\n",
    "    # Option 2: Use a fixed test_size (e.g., 100, as in original Colab)\n",
    "    # Uncomment below and comment out Option 1 if you prefer a fixed size\n",
    "    # current_test_size = config['data_config'].get('test_size', 0)\n",
    "    # if current_test_size < 10: # Or whatever threshold you prefer\n",
    "    #    config['data_config']['test_size'] = 100\n",
    "    #    print(f\"✓ Updated test_size to: {config['data_config']['test_size']}\")\n",
    "    # else:\n",
    "    #    print(f\"✓ test_size is already: {config['data_config']['test_size']}\")\n",
    "\n",
    "    # Save the updated config back to the same path (within the code dataset)\n",
    "    # Note: This modifies the config file in the /kaggle/input (read-only) if not careful.\n",
    "    # For Kaggle, it's better to load this config in the script and use these updated paths directly,\n",
    "    # OR copy the config to /kaggle/working and modify it there.\n",
    "    # The eval_kaggle.py script is designed to read these paths from the config dictionary passed to it.\n",
    "    # So, we just need to ensure the script gets this modified 'config' dictionary.\n",
    "    # The current eval_kaggle.py reads the config file itself. So we DO need to write it.\n",
    "    # Let's write it to /kaggle/working/ for this run.\n",
    "\n",
    "    KAGGLE_WORKING_DIR = \"/kaggle/working/\"\n",
    "    os.makedirs(KAGGLE_WORKING_DIR, exist_ok=True)\n",
    "    updated_config_path_in_working = os.path.join(KAGGLE_WORKING_DIR, config_filename)\n",
    "\n",
    "    with open(updated_config_path_in_working, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"✓ Updated data_path to: {config['data_config']['data_path']}\")\n",
    "    print(f\"✓ Updated json_path to: {config['data_config']['json_path']}\")\n",
    "    print(f\"✓ Updated config saved to: {updated_config_path_in_working}\")\n",
    "    print(\"The evaluation script will need to use this updated config file.\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Config file not found at {config_path_abs}. Ensure it's in your code dataset and CWD is correct.\")\n",
    "    print(\"Available files in CWD:\")\n",
    "    !ls -la *.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Data Availability\n",
    "\n",
    "Verify that the data paths (now pointing to your Kaggle data dataset) are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# KAGGLE_DATA_DATASET_PATH was defined in cell \"2. Setup Project Environment\"\n",
    "\n",
    "print(\"Checking data directories based on KAGGLE_DATA_DATASET_PATH...\")\n",
    "data_dirs_to_check = [\n",
    "    os.path.join(KAGGLE_DATA_DATASET_PATH, 'test_frames'), \n",
    "    os.path.join(KAGGLE_DATA_DATASET_PATH, 'miscellaneous')\n",
    "]\n",
    "for dir_path in data_dirs_to_check:\n",
    "    if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
    "        count = len(os.listdir(dir_path))\n",
    "        print(f\"✓ {dir_path}: Exists with {count} items\")\n",
    "    else:\n",
    "        print(f\"❌ {dir_path}: NOT FOUND or not a directory. Check KAGGLE_DATA_DATASET_PATH and dataset contents.\")\n",
    "\n",
    "print(\"\\nChecking annotation files...\")\n",
    "ann_files_to_check = [\n",
    "    os.path.join(KAGGLE_DATA_DATASET_PATH, 'miscellaneous/validation.json'), \n",
    "    os.path.join(KAGGLE_DATA_DATASET_PATH, 'miscellaneous/train.json') # Optional, for completeness\n",
    "]\n",
    "for ann_file_path in ann_files_to_check:\n",
    "    if os.path.exists(ann_file_path) and os.path.isfile(ann_file_path):\n",
    "        print(f\"✓ {ann_file_path}: EXISTS\")\n",
    "    else:\n",
    "        print(f\"❌ {ann_file_path}: NOT FOUND or not a file. Check KAGGLE_DATA_DATASET_PATH and dataset contents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "The `scripts/eval_kaggle.py` (which should be in your code dataset) will be executed.\n",
    "It will use the `base_eval_config.yaml` that we've updated and saved to `/kaggle/working/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the current working directory is still the project root from the code dataset\n",
    "# os.getcwd() should be KAGGLE_CODE_DATASET_PATH\n",
    "print(f\"Current working directory for script execution: {os.getcwd()}\")\n",
    "\n",
    "# The eval_kaggle.py script needs to be told to use the config from /kaggle/working/\n",
    "# We need to modify eval_kaggle.py to accept a config path argument, or adjust its logic.\n",
    "# For now, let's assume eval_kaggle.py is modified to look for config in /kaggle/working/ if a specific env var is set, or passed as arg.\n",
    "# The current eval_kaggle.py joins HOME_DIR with 'base_eval_config.yaml'.\n",
    "# So, the command should be run from a place where HOME_DIR is /kaggle/working/ if we want it to pick up the modified config directly,\n",
    "# OR we pass the config path as an argument to eval_kaggle.py.\n",
    "\n",
    "# Let's adjust eval_kaggle.py to accept --config_file argument.\n",
    "# (This change would need to be made to the script itself, then re-upload to dataset or modify here if written by notebook)\n",
    "# For now, the script uses os.path.join(HOME_DIR, \"base_eval_config.yaml\").\n",
    "# HOME_DIR is set to KAGGLE_CODE_DATASET_PATH.\n",
    "# So, the script will try to load /kaggle/input/mech-interp-project-code/base_eval_config.yaml (original one).\n",
    "\n",
    "# To use the MODIFIED config, we need to ensure the script loads it from /kaggle/working/base_eval_config.yaml\n",
    "# Simplest way without changing script args: copy the script to /kaggle/working, cd there, and run.\n",
    "\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working/\"\n",
    "SCRIPT_NAME = \"eval_kaggle.py\"\n",
    "SOURCE_SCRIPT_PATH = os.path.join(os.environ['HOME_DIR'], \"scripts\", SCRIPT_NAME)\n",
    "DEST_SCRIPT_PATH = os.path.join(KAGGLE_WORKING_DIR, SCRIPT_NAME) # Script in /kaggle/working/\n",
    "\n",
    "if os.path.exists(SOURCE_SCRIPT_PATH):\n",
    "    import shutil\n",
    "    shutil.copy(SOURCE_SCRIPT_PATH, DEST_SCRIPT_PATH)\n",
    "    print(f\"Copied {SCRIPT_NAME} to {DEST_SCRIPT_PATH}\")\n",
    "\n",
    "    # The config file 'base_eval_config.yaml' is already in KAGGLE_WORKING_DIR.\n",
    "    # The script eval_kaggle.py, when run from KAGGLE_WORKING_DIR and if HOME_DIR is set to KAGGLE_WORKING_DIR,\n",
    "    # will pick up config from KAGGLE_WORKING_DIR/base_eval_config.yaml.\n",
    "\n",
    "    print(\"Running evaluation script from /kaggle/working/...\")\n",
    "    # Temporarily change CWD and HOME_DIR for the script execution context\n",
    "    original_cwd = os.getcwd()\n",
    "    original_home_dir_env = os.environ.get('HOME_DIR')\n",
    "\n",
    "    os.chdir(KAGGLE_WORKING_DIR)\n",
    "    os.environ['HOME_DIR'] = KAGGLE_WORKING_DIR # So script finds config in /kaggle/working/\n",
    "    print(f\"Temporarily changed CWD to: {os.getcwd()}\")\n",
    "    print(f\"Temporarily changed HOME_DIR to: {os.environ['HOME_DIR']}\")\n",
    "\n",
    "    !python {SCRIPT_NAME} --base \n",
    "\n",
    "    # Restore original CWD and HOME_DIR\n",
    "    os.chdir(original_cwd)\n",
    "    if original_home_dir_env is not None:\n",
    "        os.environ['HOME_DIR'] = original_home_dir_env\n",
    "    else:\n",
    "        del os.environ['HOME_DIR'] # if it wasn't set before\n",
    "    print(f\"Restored CWD to: {os.getcwd()}\")\n",
    "    print(f\"Restored HOME_DIR to: {os.environ.get('HOME_DIR')}\")\n",
    "else:\n",
    "    print(f\"ERROR: Source script {SOURCE_SCRIPT_PATH} not found.\")\n",
    "\n",
    "# If you have a checkpoint to evaluate (e.g., in your code dataset under 'artifacts/my_checkpoint'):\n",
    "# CHECKPOINT_RELATIVE_PATH = \"artifacts/my_checkpoint\" # Relative to KAGGLE_CODE_DATASET_PATH\n",
    "# !python {SCRIPT_NAME} {CHECKPOINT_RELATIVE_PATH} # This would need script to handle HOME_DIR correctly for checkpoint path too.\n",
    "# The current script expects checkpoint_dir to be relative to HOME_DIR. So if HOME_DIR is /kaggle/working, this won't find it in /kaggle/input.\n",
    "# For checkpoint evaluation, eval_kaggle.py might need more robust path handling for checkpoint_dir if it's not in HOME_DIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results\n",
    "\n",
    "Results are saved in `/kaggle/working/artifacts/BASE/eval_<timestamp>/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working/\"\n",
    "RESULTS_PARENT_DIR = os.path.join(KAGGLE_WORKING_DIR, \"artifacts\", \"BASE\")\n",
    "\n",
    "if os.path.exists(RESULTS_PARENT_DIR):\n",
    "    result_dirs = glob.glob(os.path.join(RESULTS_PARENT_DIR, \"eval_*\"))\n",
    "    if result_dirs:\n",
    "        latest_dir = max(result_dirs, key=os.path.getctime)\n",
    "        print(f\"Latest results directory: {latest_dir}\")\n",
    "        \n",
    "        # Display summary results (progressive summary)\n",
    "        summary_file = os.path.join(latest_dir, \"eval_summary_progressive.txt\")\n",
    "        if os.path.exists(summary_file):\n",
    "            with open(summary_file, 'r') as f:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"EVALUATION SUMMARY (PROGRESSIVE)\")\n",
    "                print(\"=\"*50)\n",
    "                print(f.read())\n",
    "        else:\n",
    "            print(f\"Summary file not found: {summary_file}\")\n",
    "        \n",
    "        # Load and analyze detailed results (progressive details)\n",
    "        details_file = os.path.join(latest_dir, \"eval_details_progressive.json\")\n",
    "        if os.path.exists(details_file):\n",
    "            with open(details_file, 'r') as f:\n",
    "                details = json.load(f)\n",
    "            \n",
    "            print(f\"\\nTotal samples in detailed results: {len(details)}\")\n",
    "            \n",
    "            # Show some sample predictions from the detailed file\n",
    "            print(\"\\nSample predictions from detailed file (first 3):\")\n",
    "            print(\"-\" * 50)\n",
    "            for i, result in enumerate(details[:3]):\n",
    "                print(f\"\\nSample {i+1}:\")\n",
    "                print(f\"  Video: {result['video_filename']}\")\n",
    "                print(f\"  Question Type: {result['question_type']}\")\n",
    "                print(f\"  Correct: {'✓' if result['correct'] else '✗'}\")\n",
    "                if not result['correct']:\n",
    "                    print(f\"  Predicted tokens (first 10): {result['predicted_token_ids'][:10]}...\")\n",
    "                    print(f\"  Expected tokens (first 10): {result['label_token_ids'][:10]}...\")\n",
    "        else:\n",
    "            print(f\"Detailed results file not found: {details_file}\")\n",
    "    else:\n",
    "        print(f\"No 'eval_*' directories found in {RESULTS_PARENT_DIR}. Run the evaluation first!\")\n",
    "else:\n",
    "    print(f\"Results parent directory {RESULTS_PARENT_DIR} not found. Run the evaluation first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Persisting Results\n",
    "\n",
    "To save the contents of `/kaggle/working/` (including your results, logs, and the modified config):\n",
    "1. Click on `Save Version` in the Kaggle notebook editor (top right).\n",
    "2. Choose `Save & Run All (Commit)` or `Quick Save`.\n",
    "3. After the version is saved, you can find the output files in the \"Data\" tab of your notebook's viewer page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM) Error\n",
    "If you encounter OOM errors, try reducing `eval_batch_size` in `/kaggle/working/base_eval_config.yaml` (after it's copied and modified by cell 4) and re-run the evaluation cell (cell 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to reduce batch size if OOM occurs\n",
    "# This cell would be run MANUALLY if you hit OOM, then re-run cell 6 (Run Evaluation)\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working/\"\n",
    "config_filename = \"base_eval_config.yaml\"\n",
    "oom_config_path = os.path.join(KAGGLE_WORKING_DIR, config_filename)\n",
    "\n",
    "if os.path.exists(oom_config_path):\n",
    "    with open(oom_config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Set smaller batch size\n",
    "    new_batch_size = config['model_train'].get('eval_batch_size', 4) // 2\n",
    "    if new_batch_size < 1: new_batch_size = 1\n",
    "    config['model_train']['eval_batch_size'] = new_batch_size\n",
    "\n",
    "    with open(oom_config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    print(f\"✓ Batch size in {oom_config_path} reduced to {new_batch_size}. Re-run evaluation cell.\")\n",
    "else:\n",
    "    print(f\"Config file {oom_config_path} not found. Ensure cell 4 has run successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Clear GPU memory (less effective if model is already large)\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"✓ GPU memory cleared (if CUDA was available) and Python garbage collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Single Sample\n",
    "To debug with a single sample, you might need to temporarily modify `scripts/eval_kaggle.py` or create a small test script. Ensure paths point to your Kaggle data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure src is in path - os.environ['HOME_DIR'] should be /kaggle/input/mech-interp-project-code/\n",
    "sys.path.insert(0, os.environ['HOME_DIR'])\n",
    "\n",
    "from src.data import ClevrerDataset # Assuming ClevrerDataset is in src/data.py\n",
    "\n",
    "# KAGGLE_DATA_DATASET_PATH was defined in cell \"2. Setup Project Environment\"\n",
    "debug_frames_root = os.path.join(KAGGLE_DATA_DATASET_PATH, \"test_frames\")\n",
    "debug_json_path = os.path.join(KAGGLE_DATA_DATASET_PATH, \"miscellaneous/validation.json\")\n",
    "\n",
    "if os.path.exists(debug_frames_root) and os.path.exists(debug_json_path):\n",
    "    try:\n",
    "        dataset = ClevrerDataset(\n",
    "            frames_root=debug_frames_root,\n",
    "            json_path=debug_json_path,\n",
    "            question_type=\"descriptive\", # or any specific type, or 'all'\n",
    "            shuffle=False # Keep shuffle False for consistent debugging\n",
    "        )\n",
    "\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0] # Get the first sample\n",
    "            print(f\"Sample question: {sample['question']}\")\n",
    "            print(f\"Expected answer: {sample['answer']}\")\n",
    "            print(f\"Question type: {sample['question_type']}\")\n",
    "            print(f\"Number of frames: {len(sample['frames']) if 'frames' in sample and sample['frames'] is not None else 'N/A or 0'}\")\n",
    "            # print(f\"Raw item: {dataset.get_raw_item(0)}\") # If you have such a method\n",
    "        else:\n",
    "            print(\"Dataset is empty after initialization!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing or accessing dataset for debug: {e}\")\n",
    "else:\n",
    "    print(f\"Debug data paths not found. Check KAGGLE_DATA_DATASET_PATH.\")\n",
    "    print(f\"Frames root checked: {debug_frames_root}\")\n",
    "    print(f\"JSON path checked: {debug_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
