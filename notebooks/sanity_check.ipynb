{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/ai24mtech02001/miniconda3/envs/physic/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "from src.processing_paligemma import PaliGemmaProcessor\n",
    "from src.modeling_paligemma import PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data import ClevrerDataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# --- Setup ---\n",
    "model_id = \"google/paligemma2-3b-mix-448\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "activations_dir = \"../extracted_activations\"  # Directory to save activations\n",
    "os.makedirs(activations_dir, exist_ok=True)\n",
    "\n",
    "# Load model and processor\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id).to(device).eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "\n",
    "# --- Load a small subset of the CLEVRER dataset for demonstration ---\n",
    "frames_root = \"../frame_captures/\"  # Replace with the actual path to your CLEVRER frames\n",
    "json_path = \"../train.json\"  # Replace with the actual path to your CLEVRER annotations\n",
    "\n",
    "# Define transforms (adjust as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = ClevrerDataset(frames_root=frames_root, json_path=json_path, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "# Create a small subset (e.g., first sample) for demonstration\n",
    "subset_indices = [9]\n",
    "\n",
    "print(subset_indices)\n",
    "\n",
    "subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "dataloader = DataLoader(subset_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "item = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"<image> \"+item[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mframes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(torch.bfloat16).to(model.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mech_interp_physics_reasoning/src/processing_paligemma.py:297\u001b[39m, in \u001b[36mPaliGemmaProcessor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    296\u001b[39m     suffix = [sfx + \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token \u001b[38;5;28;01mfor\u001b[39;00m sfx \u001b[38;5;129;01min\u001b[39;00m suffix]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m pixel_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# max_length has to account for the image tokens\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_kwargs[\u001b[33m\"\u001b[39m\u001b[33mtext_kwargs\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_processing_utils.py:42\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/utils/generic.py:854\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m         cls_prefix = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    847\u001b[39m     warnings.warn(\n\u001b[32m    848\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    850\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    851\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    852\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:219\u001b[39m, in \u001b[36mSiglipImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m    218\u001b[39m     height, width = size[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m], size[\u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     images = \u001b[43m[\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    225\u001b[39m     images = [\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mself\u001b[39m.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n\u001b[32m    227\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    228\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:220\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m    218\u001b[39m     height, width = size[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m], size[\u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    219\u001b[39m     images = [\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    222\u001b[39m     ]\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    225\u001b[39m     images = [\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mself\u001b[39m.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n\u001b[32m    227\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    228\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:370\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL.Image.Image):\n\u001b[32m    369\u001b[39m     do_rescale = _rescale_for_pil_conversion(image)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     image = \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m height, width = size\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:197\u001b[39m, in \u001b[36mto_pil_image\u001b[39m\u001b[34m(image, do_rescale, image_mode, input_data_format)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput image type not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# If the channel has been moved to first dim, we put it back at the end.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m image = \u001b[43mto_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChannelDimension\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLAST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# If there is a single channel, we squeeze it, as otherwise PIL can't handle it.\u001b[39;00m\n\u001b[32m    200\u001b[39m image = np.squeeze(image, axis=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m image.shape[-\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:86\u001b[39m, in \u001b[36mto_channel_dimension_format\u001b[39m\u001b[34m(image, channel_dim, input_channel_dim)\u001b[39m\n\u001b[32m     84\u001b[39m     image = image.transpose((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m target_channel_dim == ChannelDimension.LAST:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     image = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported channel dimension format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: axes don't match array"
     ]
    }
   ],
   "source": [
    "\n",
    "model_inputs = processor(text=question, images=[item[\"frames\"]], return_tensors=\"pt\").to(torch.bfloat16).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Image features shape: torch.Size([8, 128, 2304])\n",
      "Input embeds shape: torch.Size([1, 8204, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:371: masked_scatter_size_check: block: [0,0,0], thread: [0,0,0] Assertion `totalElements <= srcSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mech_interp_physics_reasoning/src/modeling_paligemma.py:557\u001b[39m, in \u001b[36mPaliGemmaForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m     logger.warning_once(\n\u001b[32m    552\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`labels` contains `pad_token_id` which will be masked with `config.ignore_index`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    553\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have to mask out `pad_token_id` when preparing `labels`, this behavior will be removed in v.4.46.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     labels = torch.where(input_ids == \u001b[38;5;28mself\u001b[39m.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.ignore_index, labels)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m causal_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m outputs: CausalLMOutputWithPast = \u001b[38;5;28mself\u001b[39m.language_model(\n\u001b[32m    561\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    562\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    571\u001b[39m     **lm_kwargs,\n\u001b[32m    572\u001b[39m )\n\u001b[32m    574\u001b[39m logits = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mech_interp_physics_reasoning/src/modeling_paligemma.py:371\u001b[39m, in \u001b[36mPaliGemmaForConditionalGeneration._update_causal_mask\u001b[39m\u001b[34m(self, attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dim() == \u001b[32m4\u001b[39m:\n\u001b[32m    368\u001b[39m     \u001b[38;5;66;03m# In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m causal_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sequence_length != \u001b[32m1\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "out = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(importlib.import_module('src.activation_manager'))\n",
    "from src.activation_manager import ActivationReplacementExperiment\n",
    "\n",
    "# --- Initialize ActivationReplacementExperiment ---\n",
    "activation_experiment = ActivationReplacementExperiment(model, processor, device=device, activations_dir=activations_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m layer_names_to_extract \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# 'vision_tower.vision_model.encoder.layers.5.self_attn',\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_model.model.layers.15.self_attn.v_proj\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- Extract 'real' activations ---\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mactivation_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_names_to_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/mech_interp_physics_reasoning/activation_manager.py:68\u001b[0m, in \u001b[0;36mActivationReplacementExperiment._extract_activations\u001b[0;34m(self, dataloader, layer_names, store_as)\u001b[0m\n\u001b[1;32m     64\u001b[0m frame_set\u001b[38;5;241m.\u001b[39mappend(frames_list)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Process the batch using the PaliGemma processor\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# processed_inputs = self.processor(images=[frames_list], text=questions, return_tensors=\"pt\").to(self.device)\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_inputs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/projects/mech_interp_physics_reasoning/processing_paligemma.py:297\u001b[0m, in \u001b[0;36mPaliGemmaProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     suffix \u001b[38;5;241m=\u001b[39m [sfx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token \u001b[38;5;28;01mfor\u001b[39;00m sfx \u001b[38;5;129;01min\u001b[39;00m suffix]\n\u001b[0;32m--> 297\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# max_length has to account for the image tokens\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_processing_utils.py:42\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/utils/generic.py:866\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    863\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    864\u001b[0m     )\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:219\u001b[0m, in \u001b[0;36mSiglipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    218\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    225\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    228\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    218\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    219\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    222\u001b[0m     ]\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    225\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    228\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:369\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    368\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[0;32m--> 369\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:196\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(image, do_rescale, image_mode, input_data_format)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image type not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# If the channel has been moved to first dim, we put it back at the end.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mto_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChannelDimension\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLAST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# If there is a single channel, we squeeze it, as otherwise PIL can't handle it.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m image\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:85\u001b[0m, in \u001b[0;36mto_channel_dimension_format\u001b[0;34m(image, channel_dim, input_channel_dim)\u001b[0m\n\u001b[1;32m     83\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_channel_dim \u001b[38;5;241m==\u001b[39m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST:\n\u001b[0;32m---> 85\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported channel dimension format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Define the layer(s) to extract activations from ---\n",
    "layer_names_to_extract = [\n",
    "        # 'vision_tower.vision_model.encoder.layers.5.self_attn',\n",
    "        # 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj',\n",
    "        'language_model.model.layers.15.self_attn.q_proj',\n",
    "        'language_model.model.layers.15.self_attn.k_proj',\n",
    "        'language_model.model.layers.15.self_attn.v_proj',\n",
    "           ]\n",
    "\n",
    "\n",
    "# --- Extract 'real' activations ---\n",
    "activation_experiment._extract_activations(dataloader, layer_names_to_extract, store_as='real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8204, 2048])\n",
      "torch.Size([1, 8204, 1024])\n",
      "torch.Size([1, 8204, 1024])\n"
     ]
    }
   ],
   "source": [
    "# --- Save the extracted activations to a text file ---\n",
    "output_file = os.path.join(activations_dir, \"real_activations.txt\")\n",
    "\n",
    "for layer_name in layer_names_to_extract:  # Iterate only through the layers we extracted\n",
    "        if layer_name in activation_experiment.real_activations:\n",
    "            activations = activation_experiment.real_activations[layer_name]\n",
    "            # activations = activations.cpu().numpy().tolist()\n",
    "            print(activations.shape)\n",
    "\n",
    "\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for layer_name in layer_names_to_extract:  # Iterate only through the layers we extracted\n",
    "#         if layer_name in activation_experiment.real_activations:\n",
    "#             f.write(f\"Layer: {layer_name}\\n\")\n",
    "#             activations = activation_experiment.real_activations[layer_name]\n",
    "#             activations = activations.cpu().numpy().tolist()\n",
    "#             f.write(f\"    {activations}\\n\")\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "# print(f\"Extracted activations from specified layers saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
