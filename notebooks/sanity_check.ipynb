{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ai24mtech02001/miniconda3/envs/physic/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/ai24mtech02001/miniconda3/envs/physic/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834f8b6113af42a79d37a12979c24ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "from src.processing_paligemma import PaliGemmaProcessor\n",
    "from src.modeling_paligemma import PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data import ClevrerDataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# --- Setup ---\n",
    "model_id = \"google/paligemma2-3b-mix-448\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# activations_dir = \"../extracted_activations\"  # Directory to save activations\n",
    "# os.makedirs(activations_dir, exist_ok=True)\n",
    "\n",
    "# Load model and processor\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                        #   device_map=device,\n",
    "                                                          attn_implementation=\"eager\",\n",
    "                                                          token_compression=None,\n",
    "                                                          target_length=None).eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "\n",
    "# --- Load a small subset of the CLEVRER dataset for demonstration ---\n",
    "frames_root = \"../train_frames/\"  # Replace with the actual path to your CLEVRER frames\n",
    "json_path = \"../miscellaneous/train.json\"  # Replace with the actual path to your CLEVRER annotations\n",
    "\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = ClevrerDataset(frames_root=frames_root, json_path=json_path, transform=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:223\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transposed):\n\u001b[0;32m--> 223\u001b[0m     clone[i] \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 171\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m     {\n\u001b[0;32m--> 172\u001b[0m         key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m     }\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:235\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:236\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 236\u001b[0m                 \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:223\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transposed):\n\u001b[0;32m--> 223\u001b[0m     clone[i] \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m subset_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(full_dataset, subset_indices)\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(subset_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:191\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    181\u001b[0m                 {\n\u001b[1;32m    182\u001b[0m                     key: collate(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 }\n\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    198\u001b[0m             collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:192\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    181\u001b[0m                 {\n\u001b[1;32m    182\u001b[0m                     key: collate(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 }\n\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 192\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    194\u001b[0m         }\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    198\u001b[0m             collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:235\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    227\u001b[0m                     [\n\u001b[1;32m    228\u001b[0m                         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    229\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    230\u001b[0m                     ]\n\u001b[1;32m    231\u001b[0m                 )\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:236\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    227\u001b[0m                     [\n\u001b[1;32m    228\u001b[0m                         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    229\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    230\u001b[0m                     ]\n\u001b[1;32m    231\u001b[0m                 )\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 236\u001b[0m                 \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "# Create a small subset (e.g., first sample) for demonstration\n",
    "subset_indices = [10]\n",
    "\n",
    "print(subset_indices)\n",
    "\n",
    "subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "dataloader = DataLoader(subset_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "item = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frames': [tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5451, 0.5451, 0.5451,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5451, 0.5451, 0.5451,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5451, 0.5451, 0.5451,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5333, 0.5333, 0.5333,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5333, 0.5333, 0.5333,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5333, 0.5333, 0.5333,  ..., 0.4980, 0.4980, 0.4980]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5098, 0.5098, 0.5098],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5098, 0.5098, 0.5098],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5098, 0.5098, 0.5098]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4941, 0.4941, 0.4941],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4941, 0.4941, 0.4941],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4941, 0.4941, 0.4941]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980]]]]),\n",
       "  tensor([[[[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137],\n",
       "            [0.5412, 0.5412, 0.5412,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176],\n",
       "            [0.5373, 0.5373, 0.5373,  ..., 0.5176, 0.5176, 0.5176]],\n",
       "  \n",
       "           [[0.4000, 0.4039, 0.4078,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            [0.4000, 0.4000, 0.4039,  ..., 0.4275, 0.4314, 0.4353],\n",
       "            ...,\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980],\n",
       "            [0.5294, 0.5294, 0.5294,  ..., 0.4980, 0.4980, 0.4980]]]])],\n",
       " 'question_id': tensor([10]),\n",
       " 'question_type': ['descriptive'],\n",
       " 'question': ['Are there any stationary red objects when the video begins?'],\n",
       " 'answer': ['yes'],\n",
       " 'video_filename': ['video_01498.mp4']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"<image> \"+item[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mframes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(torch.bfloat16).to(model.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mech_interp_physics_reasoning/src/processing_paligemma.py:297\u001b[39m, in \u001b[36mPaliGemmaProcessor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    296\u001b[39m     suffix = [sfx + \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token \u001b[38;5;28;01mfor\u001b[39;00m sfx \u001b[38;5;129;01min\u001b[39;00m suffix]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m pixel_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# max_length has to account for the image tokens\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_kwargs[\u001b[33m\"\u001b[39m\u001b[33mtext_kwargs\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_processing_utils.py:42\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/utils/generic.py:854\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m         cls_prefix = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    847\u001b[39m     warnings.warn(\n\u001b[32m    848\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    850\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    851\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    852\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:219\u001b[39m, in \u001b[36mSiglipImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m    218\u001b[39m     height, width = size[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m], size[\u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     images = \u001b[43m[\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    225\u001b[39m     images = [\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mself\u001b[39m.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n\u001b[32m    227\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    228\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:220\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[32m    218\u001b[39m     height, width = size[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m], size[\u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    219\u001b[39m     images = [\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    222\u001b[39m     ]\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    225\u001b[39m     images = [\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mself\u001b[39m.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n\u001b[32m    227\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[32m    228\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:370\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL.Image.Image):\n\u001b[32m    369\u001b[39m     do_rescale = _rescale_for_pil_conversion(image)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     image = \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m height, width = size\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:197\u001b[39m, in \u001b[36mto_pil_image\u001b[39m\u001b[34m(image, do_rescale, image_mode, input_data_format)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput image type not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# If the channel has been moved to first dim, we put it back at the end.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m image = \u001b[43mto_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChannelDimension\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLAST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# If there is a single channel, we squeeze it, as otherwise PIL can't handle it.\u001b[39;00m\n\u001b[32m    200\u001b[39m image = np.squeeze(image, axis=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m image.shape[-\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:86\u001b[39m, in \u001b[36mto_channel_dimension_format\u001b[39m\u001b[34m(image, channel_dim, input_channel_dim)\u001b[39m\n\u001b[32m     84\u001b[39m     image = image.transpose((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m target_channel_dim == ChannelDimension.LAST:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     image = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported channel dimension format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: axes don't match array"
     ]
    }
   ],
   "source": [
    "\n",
    "model_inputs = processor(text=question, images=[item[\"frames\"]], return_tensors=\"pt\").to(torch.bfloat16).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Image features shape: torch.Size([8, 128, 2304])\n",
      "Input embeds shape: torch.Size([1, 8204, 2304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:371: masked_scatter_size_check: block: [0,0,0], thread: [0,0,0] Assertion `totalElements <= srcSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mech_interp_physics_reasoning/src/modeling_paligemma.py:557\u001b[39m, in \u001b[36mPaliGemmaForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m     logger.warning_once(\n\u001b[32m    552\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`labels` contains `pad_token_id` which will be masked with `config.ignore_index`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    553\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have to mask out `pad_token_id` when preparing `labels`, this behavior will be removed in v.4.46.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     labels = torch.where(input_ids == \u001b[38;5;28mself\u001b[39m.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.ignore_index, labels)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m causal_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m outputs: CausalLMOutputWithPast = \u001b[38;5;28mself\u001b[39m.language_model(\n\u001b[32m    561\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    562\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    571\u001b[39m     **lm_kwargs,\n\u001b[32m    572\u001b[39m )\n\u001b[32m    574\u001b[39m logits = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mech_interp_physics_reasoning/src/modeling_paligemma.py:371\u001b[39m, in \u001b[36mPaliGemmaForConditionalGeneration._update_causal_mask\u001b[39m\u001b[34m(self, attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dim() == \u001b[32m4\u001b[39m:\n\u001b[32m    368\u001b[39m     \u001b[38;5;66;03m# In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m causal_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sequence_length != \u001b[32m1\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "out = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(importlib.import_module('src.activation_manager'))\n",
    "from src.activation_manager import ActivationReplacementExperiment\n",
    "\n",
    "# --- Initialize ActivationReplacementExperiment ---\n",
    "activation_experiment = ActivationReplacementExperiment(model, processor, device=device, activations_dir=activations_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m layer_names_to_extract \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# 'vision_tower.vision_model.encoder.layers.5.self_attn',\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_model.model.layers.15.self_attn.v_proj\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- Extract 'real' activations ---\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mactivation_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_names_to_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/mech_interp_physics_reasoning/activation_manager.py:68\u001b[0m, in \u001b[0;36mActivationReplacementExperiment._extract_activations\u001b[0;34m(self, dataloader, layer_names, store_as)\u001b[0m\n\u001b[1;32m     64\u001b[0m frame_set\u001b[38;5;241m.\u001b[39mappend(frames_list)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Process the batch using the PaliGemma processor\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# processed_inputs = self.processor(images=[frames_list], text=questions, return_tensors=\"pt\").to(self.device)\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_inputs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/projects/mech_interp_physics_reasoning/processing_paligemma.py:297\u001b[0m, in \u001b[0;36mPaliGemmaProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     suffix \u001b[38;5;241m=\u001b[39m [sfx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token \u001b[38;5;28;01mfor\u001b[39;00m sfx \u001b[38;5;129;01min\u001b[39;00m suffix]\n\u001b[0;32m--> 297\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# max_length has to account for the image tokens\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_processing_utils.py:42\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/utils/generic.py:866\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    863\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    864\u001b[0m     )\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:219\u001b[0m, in \u001b[0;36mSiglipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    218\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    225\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    228\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    218\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    219\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    222\u001b[0m     ]\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    225\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    228\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:369\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    368\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[0;32m--> 369\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:196\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(image, do_rescale, image_mode, input_data_format)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image type not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# If the channel has been moved to first dim, we put it back at the end.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mto_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChannelDimension\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLAST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# If there is a single channel, we squeeze it, as otherwise PIL can't handle it.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m image\n",
      "File \u001b[0;32m~/miniconda3/envs/physic/lib/python3.11/site-packages/transformers/image_transforms.py:85\u001b[0m, in \u001b[0;36mto_channel_dimension_format\u001b[0;34m(image, channel_dim, input_channel_dim)\u001b[0m\n\u001b[1;32m     83\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_channel_dim \u001b[38;5;241m==\u001b[39m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST:\n\u001b[0;32m---> 85\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported channel dimension format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Define the layer(s) to extract activations from ---\n",
    "layer_names_to_extract = [\n",
    "        # 'vision_tower.vision_model.encoder.layers.5.self_attn',\n",
    "        # 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj',\n",
    "        'language_model.model.layers.15.self_attn.q_proj',\n",
    "        'language_model.model.layers.15.self_attn.k_proj',\n",
    "        'language_model.model.layers.15.self_attn.v_proj',\n",
    "           ]\n",
    "\n",
    "\n",
    "# --- Extract 'real' activations ---\n",
    "activation_experiment._extract_activations(dataloader, layer_names_to_extract, store_as='real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8204, 2048])\n",
      "torch.Size([1, 8204, 1024])\n",
      "torch.Size([1, 8204, 1024])\n"
     ]
    }
   ],
   "source": [
    "# --- Save the extracted activations to a text file ---\n",
    "output_file = os.path.join(activations_dir, \"real_activations.txt\")\n",
    "\n",
    "for layer_name in layer_names_to_extract:  # Iterate only through the layers we extracted\n",
    "        if layer_name in activation_experiment.real_activations:\n",
    "            activations = activation_experiment.real_activations[layer_name]\n",
    "            # activations = activations.cpu().numpy().tolist()\n",
    "            print(activations.shape)\n",
    "\n",
    "\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for layer_name in layer_names_to_extract:  # Iterate only through the layers we extracted\n",
    "#         if layer_name in activation_experiment.real_activations:\n",
    "#             f.write(f\"Layer: {layer_name}\\n\")\n",
    "#             activations = activation_experiment.real_activations[layer_name]\n",
    "#             activations = activations.cpu().numpy().tolist()\n",
    "#             f.write(f\"    {activations}\\n\")\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "# print(f\"Extracted activations from specified layers saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ai24mtech02001/miniconda3/envs/physic/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/ai24mtech02001/miniconda3/envs/physic/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 3557 unique videos: 3547 for train, 10 for test.\n",
      "Split generated and saved to cache: miscellaneous/split_indices.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "from src.data import ClevrerDataset\n",
    "\n",
    "test_frames_dir = \"../test_frames\"\n",
    "annotations_path = \"../miscellaneous/validation.json\"\n",
    "\n",
    "dataset = ClevrerDataset(\n",
    "    frames_root=test_frames_dir,\n",
    "    json_path=annotations_path,\n",
    "    question_type=\"predictive\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "train_ds, test_ds = dataset.train_test_split(test_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frames': [<PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>],\n",
       " 'question_id': 12,\n",
       " 'question_type': 'predictive',\n",
       " 'question': 'What will happen next? a) The gray object collides with the sphere b) The gray cylinder collides with the yellow cylinder',\n",
       " 'answer': 'a',\n",
       " 'video_filename': 'video_12738.mp4'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [01:44<00:00, 34.83s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:46<00:00, 15.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaliGemmaForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(256, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "    (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
       "  )\n",
       "  (language_model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(257216, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): GemmaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=257216, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "from src.processing_paligemma import PaliGemmaProcessor\n",
    "from src.modeling_paligemma import PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data import ClevrerDataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import yaml\n",
    "from peft import PeftModel, PeftConfig\n",
    "# --- Setup ---\n",
    "\n",
    "\n",
    "pretrained_path = \"artifacts/Caravaggio_1571\"\n",
    "pretrained_path = os.path.join(\"/data1/ai24mtech02001/projects/mech_interp_physics_reasoning\", pretrained_path)\n",
    "\n",
    "config_path = os.path.join(pretrained_path, \"config.yaml\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "\n",
    "model_id = \"google/paligemma-3b-mix-224\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# activations_dir = \"../extracted_activations\"  # Directory to save activations\n",
    "# os.makedirs(activations_dir, exist_ok=True)\n",
    "\n",
    "# Load model and processor\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                        #   device_map=device,\n",
    "                                                          attn_implementation=\"eager\",\n",
    "                                                          # token_compression=config[\"model_train\"][\"token_compression\"],\n",
    "                                                          # target_length=config[\"model_train\"][\"target_length\"]\n",
    "                                                          ).eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# lora_model_path = \"/data1/ai24mtech02001/projects/mech_interp_physics_reasoning/artifacts/Caravaggio_1571/checkpoint-8000\"\n",
    "# model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# # --- Load a small subset of the CLEVRER dataset for demonstration ---\n",
    "# frames_root = \"../train_frames/\"  # Replace with the actual path to your CLEVRER frames\n",
    "# json_path = \"../miscellaneous/train.json\"  # Replace with the actual path to your CLEVRER annotations\n",
    "\n",
    "\n",
    "# # Load the full dataset\n",
    "# full_dataset = ClevrerDataset(frames_root=frames_root, json_path=json_path, transform=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load a small subset of the CLEVRER dataset for demonstration ---\n",
    "frames_root = \"../test_frames/\"  # Replace with the actual path to your CLEVRER frames\n",
    "json_path = \"../miscellaneous/validation.json\"  # Replace with the actual path to your CLEVRER annotations\n",
    "\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = ClevrerDataset(frames_root=frames_root, \n",
    "                              json_path=json_path,\n",
    "                              question_type=\"predictive\", \n",
    "                              transform=None)\n",
    "\n",
    "train_ds, test_ds = full_dataset.train_test_split(test_size=10)\n",
    "\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frames': [<PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>,\n",
       "  <PIL.Image.Image image mode=RGB size=480x320>],\n",
       " 'question_id': 13,\n",
       " 'question_type': 'predictive',\n",
       " 'question': 'Which event will happen next? a) The metal cube collides with the purple cylinder b) The metal cylinder and the purple object collide',\n",
       " 'answer': 'b',\n",
       " 'video_filename': 'video_10763.mp4'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer en Frame 1: <image> # Frame 2: <image> How many objects have entered Frame 2?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# text = \"Select all that apply. \"+ test_ds[0][\"question\"] + \"<image> \" * int(8 * config[\"model_train\"][\"target_length\"]/1024)\n",
    "# text = \"<image> \" * 4 + \"Name 5 colors\"\n",
    "# text = \"answer en <image> <image> How many objects are in these images?\"\n",
    "text = \"answer en \\\n",
    "Frame 1: <image> \\\n",
    "Frame 2: <image> \\\n",
    "How many objects have entered Frame 2?\\n\"\n",
    "print(text)\n",
    "\n",
    "image_path_1 = \"/data1/ai24mtech02001/projects/mech_interp_physics_reasoning/train_frames/video_00000-01000/video_00000/frame_0.jpg\"\n",
    "image_path_2 = \"/data1/ai24mtech02001/projects/mech_interp_physics_reasoning/train_frames/video_00000-01000/video_00000/frame_1.jpg\"\n",
    "image_1 = Image.open(image_path_1)\n",
    "image_2 = Image.open(image_path_2)\n",
    "\n",
    "# inputs = processor(images=[test_ds[0][\"frames\"]], text=text, return_tensors=\"pt\").to(model.dtype).to(model.device)\n",
    "# inputs = processor(images=[image_1], text=text, return_tensors=\"pt\").to(model.dtype).to(model.device)\n",
    "inputs = processor(images=[[image_1, image_2]], text=text, return_tensors=\"pt\").to(model.dtype).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 13072,    659, 235248,  ...,   2416, 235336,    108]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[[[-0.2000, -0.1922, -0.1843,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          [-0.2000, -0.2000, -0.1922,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          [-0.2000, -0.2000, -0.1922,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          ...,\n",
       "          [ 0.0824,  0.0824,  0.0824,  ...,  0.0275,  0.0275,  0.0275],\n",
       "          [ 0.0824,  0.0824,  0.0824,  ...,  0.0275,  0.0275,  0.0275],\n",
       "          [ 0.0824,  0.0824,  0.0824,  ...,  0.0275,  0.0275,  0.0275]],\n",
       "\n",
       "         [[-0.2000, -0.1922, -0.1843,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          [-0.2000, -0.2000, -0.1922,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          [-0.2000, -0.2000, -0.1922,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          ...,\n",
       "          [ 0.0745,  0.0745,  0.0745,  ...,  0.0353,  0.0353,  0.0353],\n",
       "          [ 0.0745,  0.0745,  0.0745,  ...,  0.0353,  0.0353,  0.0353],\n",
       "          [ 0.0745,  0.0745,  0.0745,  ...,  0.0353,  0.0353,  0.0353]],\n",
       "\n",
       "         [[-0.2000, -0.1922, -0.1843,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          [-0.2000, -0.2000, -0.1922,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          [-0.2000, -0.2000, -0.1922,  ..., -0.1451, -0.1373, -0.1294],\n",
       "          ...,\n",
       "          [ 0.0588,  0.0588,  0.0588,  ..., -0.0039, -0.0039, -0.0039],\n",
       "          [ 0.0588,  0.0588,  0.0588,  ..., -0.0039, -0.0039, -0.0039],\n",
       "          [ 0.0588,  0.0588,  0.0588,  ..., -0.0039, -0.0039, -0.0039]]]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaliGemmaCausalLMOutputWithPast(loss=None, logits=tensor([[[ -7.9757,  -1.3099, -14.6626,  ...,  -5.6320,  -5.6418,  -5.6357],\n",
       "         [ -9.7266,   5.2611, -13.3249,  ...,  -6.4588,  -6.4680,  -6.4640],\n",
       "         [-12.6550,  -2.2570, -21.8329,  ...,  -8.2846,  -8.2754,  -8.2684],\n",
       "         ...,\n",
       "         [ -7.2076,   3.7804, -13.0085,  ...,  -4.3190,  -4.3376,  -4.3337],\n",
       "         [-13.2116,  20.4997, -11.7819,  ...,  -6.1259,  -6.1417,  -6.1394],\n",
       "         [ -7.3759,   3.8437,  -9.0900,  ...,  -5.9371,  -5.9442,  -5.9315]]],\n",
       "       device='cuda:0'), past_key_values=<transformers.cache_utils.HybridCache object at 0x7b4d4cad7a90>, hidden_states=None, attentions=None, image_hidden_states=tensor([[[ 1.5772e-05, -9.4527e-03, -5.5193e-03,  ..., -3.9842e-03,\n",
       "          -5.3947e-03, -7.4642e-03],\n",
       "         [ 9.2647e-04,  2.0819e-03, -1.4647e-03,  ..., -9.3158e-04,\n",
       "          -7.1842e-03,  5.3776e-03],\n",
       "         [-1.1814e-03, -3.9834e-03,  2.0918e-04,  ..., -3.9332e-03,\n",
       "          -6.0337e-03,  3.5331e-03],\n",
       "         ...,\n",
       "         [-1.3729e-02,  2.0089e-02, -4.9609e-03,  ..., -1.7099e-03,\n",
       "           6.4533e-04,  1.4572e-03],\n",
       "         [-2.9394e-03,  1.0947e-02, -4.1689e-03,  ..., -7.0484e-03,\n",
       "           4.0461e-03, -4.3719e-03],\n",
       "         [ 1.0454e-02, -1.9421e-03, -5.0812e-03,  ...,  5.2453e-03,\n",
       "          -7.5317e-03,  3.5584e-03]],\n",
       "\n",
       "        [[ 9.8680e-04, -4.7106e-03, -7.7593e-03,  ..., -7.9596e-03,\n",
       "          -7.6491e-03, -1.0358e-02],\n",
       "         [-3.6962e-03,  1.0115e-02,  1.2031e-03,  ...,  5.8645e-03,\n",
       "          -3.0135e-03, -1.5812e-03],\n",
       "         [-6.8652e-03, -9.1487e-04,  5.4766e-03,  ..., -5.5735e-03,\n",
       "           5.9451e-04, -2.8629e-03],\n",
       "         ...,\n",
       "         [-1.5204e-02,  1.9191e-02, -2.3772e-03,  ..., -3.8686e-04,\n",
       "          -7.2498e-04,  2.1617e-03],\n",
       "         [-2.8996e-03,  5.5253e-03, -6.8492e-03,  ..., -7.3434e-03,\n",
       "           3.3731e-03, -1.3273e-03],\n",
       "         [ 8.7950e-03, -1.2391e-03, -5.5765e-03,  ...,  5.1478e-03,\n",
       "          -7.4715e-03,  3.6045e-03]],\n",
       "\n",
       "        [[ 1.7447e-03, -1.9406e-03, -6.5977e-03,  ..., -3.8627e-03,\n",
       "          -5.1239e-03, -8.4121e-03],\n",
       "         [-3.5323e-03,  1.0017e-02,  1.2180e-03,  ...,  5.6141e-03,\n",
       "          -3.5387e-03, -1.9258e-03],\n",
       "         [-6.7385e-03,  4.0081e-03, -1.3764e-04,  ..., -3.5532e-03,\n",
       "           3.0348e-03, -5.2008e-03],\n",
       "         ...,\n",
       "         [-1.2875e-02,  1.8615e-02, -3.8859e-03,  ..., -2.3895e-03,\n",
       "           1.2685e-03,  1.7472e-03],\n",
       "         [-5.5384e-03,  1.1661e-02, -6.8585e-05,  ..., -1.3613e-02,\n",
       "           3.5998e-03,  7.8348e-04],\n",
       "         [ 5.8487e-03, -1.5280e-03, -5.0903e-03,  ...,  3.1866e-03,\n",
       "          -5.8663e-03,  1.8493e-03]],\n",
       "\n",
       "        [[ 1.9146e-03, -3.9543e-03, -5.4782e-03,  ..., -4.9802e-03,\n",
       "          -6.9050e-03, -7.9767e-03],\n",
       "         [-3.7793e-03,  1.0733e-02,  7.1932e-04,  ...,  6.2705e-03,\n",
       "          -3.1370e-03, -2.2803e-03],\n",
       "         [-1.6019e-03,  7.4413e-04,  8.4092e-04,  ..., -1.5993e-03,\n",
       "          -2.6070e-03,  4.6200e-03],\n",
       "         ...,\n",
       "         [-1.0409e-02,  1.6307e-02, -3.0128e-03,  ..., -2.0203e-03,\n",
       "           1.4325e-03,  1.3335e-03],\n",
       "         [-4.1764e-03,  8.0524e-03, -2.3304e-03,  ..., -9.2097e-03,\n",
       "           2.9059e-03,  1.0417e-03],\n",
       "         [ 1.0626e-02, -5.2397e-03, -4.4415e-03,  ...,  6.8039e-03,\n",
       "          -8.0959e-03,  4.0140e-03]]], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 537])\n",
      "tensor([[ 13072,    659,  17400, 235248, 235274, 235292, 235248, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152,  17400, 235248, 235284, 235292, 235248, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
      "         257152, 257152,      2,   2250,   1767,   9113,    791,  10783,  17400,\n",
      "         235248, 235284, 235336,    109, 235274,      1]], device='cuda:0')\n",
      "answer en Frame 1:  Frame 2:  How many objects have entered Frame 2?\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    print(generation.shape)\n",
    "    print(generation)\n",
    "    generation = generation[0]\n",
    "    # generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = generation[input_len:]\n",
    "decoded = processor.decode(generation)\n",
    "print(decoded)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and endearing nature of the situation. The baby is wearing glasses and appears to be reading a book, which is a humorous and endearing sight because babies are typically not expected to be able to read at such a young age.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "\n",
    "# Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos)\n",
    "video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n",
    "            {\"type\": \"video\", \"path\": video_path},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=60)\n",
    "processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
